{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import sys\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import csv\n",
    "\n",
    "import shelve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from datetime import datetime\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "td = timedelta (days = 0, hours =  1, minutes =  36) # time diff between laptop and realtime\n",
    "Anfang = datetime.now()\n",
    "Start = Anfang + td"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "import random\n",
    "random.seed (1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import liwc\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "LIWC_dictionary = '/home/xhta/Robot/liwc/timeori.dic'   ##   file cotaining the LIWC categories, need to set before run\n",
    "\n",
    "parse, cat_names = liwc.load_token_parser(LIWC_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tp_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "translator = str.maketrans(' ', ' ', punctuation)\n",
    "from nltk.corpus import stopwords\n",
    "stoplist = set(stopwords.words('english'))\n",
    "from nltk.stem import SnowballStemmer\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "nlp3 = spacy.load('en_core_web_sm')\n",
    "\n",
    "matcher = Matcher(nlp3.vocab)\n",
    "\n",
    "def normalize_text(doc):         # \n",
    "    tokens = []\n",
    "    for sent in doc.sents:\n",
    "        sent = str(sent)\n",
    "        sent = sent.replace('\\r', ' ').replace('\\n', ' ')\n",
    "        lower = sent.lower()\n",
    "        nopunc = lower.translate(translator)\n",
    "        words = nopunc.split()\n",
    "        nostop = [w for w in words if w not in stoplist]\n",
    "        no_numbers = [w if not w.isdigit() else '#' for w in nostop]\n",
    "        stemmed = [stemmer.stem(w) for w in no_numbers]\n",
    "        tokens += stemmed\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpath = '/home/xhta/Robot/cases/'       # where the cases are located in file system , need to set before run\n",
    "\n",
    "sample_size = 1.0  # fraction of the whole corpis used for sampling !!!!!!!!!!!!!!!!!!!!!!\n",
    "\n",
    "cases_metadata_pickle = '/home/xhta/Robot/cases_metadata.20190627_0039.pkl'     # where the cases metadata are located in file system , need to set before run\n",
    "\n",
    "import pickle\n",
    "df = pickle.load(open(cases_metadata_pickle, \"rb\"))\n",
    "df = df.sample(frac=sample_size)\n",
    "\n",
    "ldir = listdir(fpath)\n",
    "\n",
    "import spacy    #  initializing data structures used later\n",
    "from spacy.lang.en import English\n",
    "nlp = English()\n",
    "nlp.add_pipe(nlp.create_pipe('sentencizer'))\n",
    "\n",
    "nlp2 = spacy.load('en_core_web_sm')\n",
    "\n",
    "n_sandglass = 299        # displays on console, useful for long runs\n",
    "cp_interval = 179      # checkpoint time interval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# populate dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>case_reversed</th>\n",
       "      <th>judge_id</th>\n",
       "      <th>year</th>\n",
       "      <th>log_cites</th>\n",
       "      <th>LastName</th>\n",
       "      <th>FirstName</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Pres</th>\n",
       "      <th>Party</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>caseid</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>X3JGGO</th>\n",
       "      <td>0</td>\n",
       "      <td>1653</td>\n",
       "      <td>1925</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>Miller</td>\n",
       "      <td>Wilbur</td>\n",
       "      <td>1</td>\n",
       "      <td>Harry S Truman</td>\n",
       "      <td>Democratic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>X3OH3J</th>\n",
       "      <td>0</td>\n",
       "      <td>1034</td>\n",
       "      <td>1924</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>Hickenlooper</td>\n",
       "      <td>Smith</td>\n",
       "      <td>1</td>\n",
       "      <td>Warren G. Harding</td>\n",
       "      <td>Republican</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>X3U0KO</th>\n",
       "      <td>0</td>\n",
       "      <td>2303</td>\n",
       "      <td>1925</td>\n",
       "      <td>1.791759</td>\n",
       "      <td>Story</td>\n",
       "      <td>William</td>\n",
       "      <td>1</td>\n",
       "      <td>Ulysses Grant</td>\n",
       "      <td>Republican</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>X53HAD</th>\n",
       "      <td>0</td>\n",
       "      <td>1764</td>\n",
       "      <td>1924</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Nielsen</td>\n",
       "      <td>Leland</td>\n",
       "      <td>1</td>\n",
       "      <td>Richard M. Nixon</td>\n",
       "      <td>Republican</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>X9VC5V</th>\n",
       "      <td>0</td>\n",
       "      <td>493</td>\n",
       "      <td>1925</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Connally</td>\n",
       "      <td>Ben</td>\n",
       "      <td>1</td>\n",
       "      <td>Harry S Truman</td>\n",
       "      <td>Democratic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        case_reversed  judge_id  year  log_cites      LastName FirstName  \\\n",
       "caseid                                                                     \n",
       "X3JGGO              0      1653  1925   1.098612        Miller    Wilbur   \n",
       "X3OH3J              0      1034  1924   1.609438  Hickenlooper     Smith   \n",
       "X3U0KO              0      2303  1925   1.791759         Story   William   \n",
       "X53HAD              0      1764  1924   0.000000       Nielsen    Leland   \n",
       "X9VC5V              0       493  1925   0.000000      Connally       Ben   \n",
       "\n",
       "        Gender               Pres       Party  \n",
       "caseid                                         \n",
       "X3JGGO       1     Harry S Truman  Democratic  \n",
       "X3OH3J       1  Warren G. Harding  Republican  \n",
       "X3U0KO       1      Ulysses Grant  Republican  \n",
       "X53HAD       1   Richard M. Nixon  Republican  \n",
       "X9VC5V       1     Harry S Truman  Democratic  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for fname in ldir:      # read in the document samples and populate the dataframe with linguistic features\n",
    "    if isfile(fname) == 'False': continue\n",
    "    lae = len(fname)\n",
    "    cname = fname[5:(lae-4)]\n",
    "    year = fname[0:4]\n",
    "\n",
    "    if (not (cname in df.index)): continue # useful if only using a sample of cases\n",
    "\n",
    "    if ( i% n_sandglass==0):\n",
    "        print((datetime.now() + td).strftime('%Y%m%d_%H:%M:%S'), \"  i:\", i, \"  running script:\", sys.argv[0])\n",
    "#\n",
    "    fna2 = join(fpath, year + '_' + cname + '.txt')\n",
    "    rawtext = open(fna2).read()\n",
    "    rawtext = rawtext.replace('\\n', '')\n",
    "    rawtext = rawtext.replace(\"\\'s\", \"'s\")\n",
    "\n",
    "    doc = nlp2(rawtext)\n",
    "    df.at[cname, 'nlets'] =  len(rawtext)\n",
    "    sentences = [sent.string.strip() for sent in doc.sents]\n",
    "    df.at[cname, 'nsents'] =  len(sentences)\n",
    "    df.at[cname, 'nwords'] = len([token for token in doc if not token.is_punct])\n",
    "\n",
    "    df.at[cname, 'doc'] = rawtext\n",
    "#\n",
    "    cltoks = normalize_text(doc)\n",
    "    ntoks = [str(token).lower() for token in list(doc) if (not token.is_punct) & (not token.is_space) & (not token.is_stop) & (str(token) in cltoks)]\n",
    "#\n",
    "    df.at[cname, 'ntoks'] = len(ntoks)\n",
    "\n",
    "    df.at[cname, 'nverbs'] = len([w for w in list(doc) if w.tag_.startswith('V')])\n",
    "\n",
    "    npast, npresent, nfuture, antpast, antpresent, antfuture = spacy_parse(doc)         # get the past, present, and future tenses based on spacy's tagging\n",
    "#\n",
    "    df.loc[cname, 'npast'] = npast                      # npast, npresent and nfuture are numbers indicating the frequencies of past, present and future tenses in the doc\n",
    "    df.loc[cname, 'npresent'] = npresent\n",
    "    df.loc[cname, 'nfuture'] = nfuture\n",
    "    df.loc[cname, 'antpast'] = antpast                  # antpast = npast / (npast + npesent + nfuture)\n",
    "    df.loc[cname, 'antpresent'] = antpresent\n",
    "    df.loc[cname, 'antfuture'] = antfuture\n",
    "\n",
    "    nfpast, nfpresent, nffuture, antfpast, antfpresent, antffuture = liwc_parse(rawtext)        # get the past, present and future focus using LIWC\n",
    "#\n",
    "    df.loc[cname, 'nfpast'] = nfpast\n",
    "    df.loc[cname, 'nfpresent'] = nfpresent\n",
    "    df.loc[cname, 'nffuture'] = nffuture\n",
    "\n",
    "    df.loc[cname, 'antfpast'] = antfpast                # antfpast = nfpast / (nfpast + nfpresent + nffuture)\n",
    "    df.loc[cname, 'antfpresent'] = antfpresent\n",
    "    df.loc[cname, 'antffuture'] = antffuture\n",
    "\n",
    "    lse = nltk.sent_tokenize(rawtext)                   # break up the rawtext into a list of sentences and parse the sentences individually\n",
    "    ldeont = 0                                          # to find the frequencies of deontic futures and of modal verbs (would could might)\n",
    "    lmodal = 0\n",
    "    for j in range(len(lse)):                           # both deont_parse and modal_parse are spacy-based and located in tp_utils.py\n",
    "        ldeont += deont_parse(lse[j])\n",
    "        lmodal += modal_parse(lse[j])\n",
    "    df.at[cname, \"ldeont\"] = ldeont\n",
    "    df.at[cname, \"lmodal\"] = lmodal\n",
    "#\n",
    "    if (i % cp_interval  == 0):                         # checkpointing in intervals\n",
    "        je = datetime.now() + td\n",
    "        pkl_fname = 'pj_df_full.' + je.strftime('%Y%m%d_%H%M%S' + \".pkl\")\n",
    "        print (\"jetzt:{0}  i:{1} ... intermediate checkpointing to {2}\".format(je, i, pkl_fname))\n",
    "        df.to_pickle(pkl_fname)\n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "je = datetime.now() + td\n",
    "pkl_fname = 'pj_df_full.' + je.strftime('%Y%m%d_%H%M%S' + \".pkl\")\n",
    "print (\"jetzt:{0}  i:{1} ... final write to pickle {2}\".format(je, i, pkl_fname))\n",
    "df.to_pickle(pkl_fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# make a plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a plot showing the proportions of past, present and future tenses averaged across the entire corpus\n",
    "sAvepast = df[\"antpast\"].mean()                 # spacy \n",
    "sAvepresent = df[\"antpresent\"].mean()\n",
    "sAvezfuture = df[\"antfuture\"].mean()\n",
    "\n",
    "lAvepast = df[\"antfpast\"].mean()                # LIWC         \n",
    "lAvepresent = df[\"antfpresent\"].mean()\n",
    "lAvezfuture = df[\"antffuture\"].mean()\n",
    "\n",
    "modfplot = pd.DataFrame({'Avepast' : [lAvepast, sAvepast], 'Avepresent': [lAvepresent, sAvepresent], 'Avezfuture': [lAvezfuture, sAvezfuture]}, index =\n",
    " ['LIWC', 'Spacy'] )\n",
    "modfplot.plot.bar(rot=0)\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
